#+startup: beamer content

#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: Inference in Haskell for ABC
#+author: Piotr Kozicki
#+email: piotr.kozicki.2022@bristol.ac.uk
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+cite_export:

#+options: H:2
#+latex_class: beamer
#+latex_compiler: xelatex
#+latex_header: \usepackage{fontspec}
#+latex_header: \setsansfont{Fira Sans}
#+latex_header: \setmonofont{Fira Code}[Contextuals=Alternate]
#+latex_header: \usepackage{pgfplots}
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: CambridgeUS
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

# REFERENCE to insert graphics later
# #+latex_header: \titlegraphic{\includegraphics{Rplots}}

* Approximate Bayesian Computation
** Bayesian Inference

#+attr_latex: :options [Bayes' Theorem]
#+begin_theorem
\( p(a|b)p(b) = p(b|a)p(a) \).
#+end_theorem

Often rewritten as

\[
\overbrace{p(\theta | x)}^{\text{posterior}}
\propto
\overbrace{p(x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}.
\]

We get a distribution on hypotheses \theta given evidence \(x\).

** Example

Suppose \(x = (3~1~7~5~1~2~3~4~3~2)\) comes from \(Poi(\theta)\) assuming
nothing about \(\theta\).

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\begin{align*}
  p (\theta | x)
  & \propto p (x | \theta) p (\theta) \\
  & = p (x | \theta) = \prod_{i=1}^{10} p (x_i | \theta) \\
  & \propto e^{-10 \theta} \theta^{31}
\end{align*}
#+end_export

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\center
\begin{tikzpicture}[scale=0.7]
\begin{axis} [
    axis lines = left,
    xlabel = {\(\theta\)},
    ylabel = {\(k \cdot p(\theta | X)\)},
  ]
  \addplot [
    domain = 0:10,
    samples = 200,
    color = red,
  ]
  { (exp (-x * 10))
    * x^(31) };
  \addplot [color = black] coordinates {(3,0)(3,60)};
\end{axis}
\end{tikzpicture}
#+end_export

** Motivation

In this example we used a simple distribution.
In reality, the likelihood may be unavailable or intractable.

\hfill

To avoid this problem, we avoid likelihoods all together and instead consider
only generative models.

#+begin_src haskell
newtype Sampler ω = Sampler {runSampler :: ReaderT Gen IO ω}
  deriving (Functor, Applicative, Monad)

sample :: Sampler ω -> Gen -> IO ω
sample = runReaderT . runSampler
#+end_src

** ABC Description

To be able to use ABC we need:
1. A generative model \(\mu : \theta \to \texttt{Sampler}~\omega\)
2. Real observations \(\mathbf y : \omega\)

#+begin_src haskell
accept :: Eq ω => ω -> Sampler ω -> θ -> Sampler (θ, Double)
accept y μ θ = do
  x <- μ θ
  return $ (θ, if x == y then 1 else 0)
#+end_src

By repeating this, we can generate samples from the posterior \(p(\theta|\mathbf y)\).

** Sampling Methods

See that ABC describes an indirection for weighting proposals for \(\theta\).

\hfill

We still need to propose and accumulate values for \(\theta\), with a Monte Carlo method.

* Rejection Sampling
** Rejection Sampling

A simple Monte-Carlo sampling method.

Proposals are taken from a prior, then we have to decide whether or not to accept them.

#+beamer: \pause

#+begin_src haskell
class RSKernel k a | k -> a where
  propose :: k -> IO a
  accepts :: k -> a -> IO Bool
#+end_src

#+begin_src haskell
rs :: RSKernel k a => Int -> k -> IO [a]
rs 0 _ = return []
rs n k = do
  x <- propose k
  a <- k `accepts` x
  if a
    then (x:) <$> rs (n-1) k
    else rs (n-1) k
#+end_src

** Distribution Approximation

We provide a prior \(g\) we can sample from directly such that \(f \leq M \cdot g\).

#+begin_src haskell
data RSMC ω = RSMC
  { prior :: Sampler ω
  , priorDensity :: ω -> Double -- ^ scaled by M
  , targetDensity :: ω -> Double
  , gen :: Gen
  }

instance RSKernel (RSMC ω) ω where
  propose :: RSMC ω -> IO ω
  propose RSMC{..} = sample prior gen

  accepts :: RSMC ω -> ω -> IO Bool
  accepts RSMC{..} x = let
    α = targetDensity x / priorDensity x
    in sample (bernoulli $ min 1 α) gen
#+end_src

*demo*

** Approximate Bayesian Computation

#+begin_src haskell
data RSABC θ ω = RSABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , prior :: Sampler θ
  , gen :: Gen
  }

instance RSKernel (RSABC θ ω) θ where
  propose :: RSABC θ ω -> IO θ
  propose RSABC{..} = sample prior gen

  accepts :: RSABC θ ω -> θ -> IO Bool
  accepts RSABC{..} θ = do
    x <- sample (model θ) gen
    return $ x == observations
#+end_src

*demo*
* Improvements
** Tolerance

To increase the acceptance rate, we usually use a weaker condition, that
\(|| \mathbf x - \mathbf y || \leq \epsilon\).

#+begin_src haskell
RSABC θ ω = RSABC
  { distance :: ω -> ω -> Double
  , tolerance :: Double
  , ..
  }

instance RSKernel (RSABC θ ω) where
  accepts :: RSABC θ ω -> θ -> IO Bool
  accepts RSABC{..} θ = do
    x <- sample (model θ) gen
    return $ distance x y <= tolerance
#+end_src

The choice of distance function doesn't matter too much, so long as it's
sensible. A good choice might be the sum of squared distances: naturally this depends on \(\omega\).
*demo*

** Summary Statistics

We rarely compare only one sample at a time, so usually the sample space \(\omega\) will have many dimensions.

This introduces the "curse of dimensionality", which here means any two samples we generate might seem to be far apart.

#+beamer: \pause

\hfill

We try to solve this by replacing raw data with summary statistics, e.g. quantiles.

\[
S : \omega \to s ; ~ \mathbf x \mapsto (\mathbf x_{(0)}, Q_1, Q_2, Q_3, \mathbf x_{(n)})
\]

Finding informative summary statistics is one of the bigger problems in ABC.

TODO describe the two goals of summary statistics, informative and qualitative

*demo*

* TODO Metropolis-Hastings
** Metropolis-Hastings

An improvement on rejection sampling. Stay near to accepted samples by carrying
out a /random walk/ over values of \(\theta\), rather than resampling from the
prior.

#+begin_src haskell
class MHKernel k a | k -> a where
  propose :: k -> a -> IO a
  accepts :: k -> a -> a -> IO Bool
#+end_src

#+begin_src haskell
mh :: MHKernel k a => Int -> k -> a -> IO [a]
mh 0 _ _ = return []
mh n k x_0 = undefined
#+end_src

** Approximate Bayesian Computation
** Tuning
* Reading
** Reading

- [[https://www.pnas.org/doi/10.1073/pnas.0306899100][Marjoram et al]]
- [[https://www.maths.lu.se/fileadmin/maths/forskning_research/InferPartObsProcess/abc_slides.pdf][Umberto Picchini's slides on ABC]]

- [[https://arxiv.org/abs/1004.1112][Fernhead and Prangle --- Constructing Summary Statistics]]
- [[https://projecteuclid.org/journals/statistical-science/volume-28/issue-2/A-Comparative-Review-of-Dimension-Reduction-Methods-in-Approximate-Bayesian/10.1214/12-STS406.full][Blum et al --- Comparative Review of Dimension Reduction Methods]]

TODO full data methods
