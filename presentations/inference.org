#+startup: beamer content

#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: ApproximateBayesianComputation.hs
#+author: Piotr Kozicki
#+email: piotr.kozicki.2022@bristol.ac.uk
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+cite_export:

#+options: H:2
#+latex_class: beamer
#+latex_compiler: xelatex
#+latex_header: \usepackage{fontspec}
#+latex_header: \setsansfont{Fira Sans}
#+latex_header: \setmonofont{Fira Code}[Contextuals=Alternate]
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: CambridgeUS
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

# REFERENCE to insert graphics later
#+latex_header: \titlegraphic{\includegraphics{Rplots}}

# TODO code highlighting via lstlistings

* /briefly/: Bayesian Inference
** Bayes' Theorem

#+attr_latex: :options [Bayes' Theorem]
#+begin_theorem
\( p(a|b)p(b) = p(b|a)p(a) \).
#+end_theorem

#+begin_proof
TOD.
#+end_proof

Often rewritten as

\[
\overbrace{p(\theta | x)}^{\text{posterior}}
\propto
\overbrace{p(x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}.
\]

We get a distribution on hypotheses \theta given evidence \(x\).

** TODO Example

#+begin_src
xs <- (8, 2, 6, 6, 9) ~ Bin 10 θ
ASS θ ~ U(0, 1) i.e. flat prior
p(θ|x) = p(x|θ)p(θ) = p(x|θ)
#+end_src

\[
p(\theta | x)
=
\prod_{x \in \text{xs}} \binom{10}{x} \theta^x (1 - \theta)^{10-x}
\]

#+begin_src R example1 :exports results :results graphics file :file example1.png
xs <- c(8,2,6,6,9)
posterior <- function (th) {
  return(prod(mapply(\(x) (choose(10, x)) * th^x * (1 - th)^(10 - x), xs)))
}
curve(Vectorize(posterior)(x), from=0, to=1)
#+end_src

# BUG doesn't fit on the slide

** Limitation

\[
\overbrace{p(\theta | x)}^{\text{posterior}}
\propto
\overbrace{p(x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}
\]

See that we need to know the likelihood to get the posterior.

\hfill

In reality, \(p(x | \theta)\) can be very complicated or intractable.

\hfill

We can simplify our models or we can avoid likelihoods all together.

* Approximate Bayesian Computation
** ABC Overview

Approximate Bayesian Computation requires:
- a model \mu : \Theta \rightarrow =Dist= \Omega
- observations \(\mathbf y\) : \Omega.

#+beamer: \pause

In gist:
1. Take some \theta : \Theta.
2. Take \(\mathbf x\) : \Omega \leftarrow \mu(\theta).
3. If \(\mathbf x\) is "close" to \(\mathbf y\), say \theta is close to \theta*.

#+beamer: \pause

This is repeated a large number of times so we can approximate a distribution for \(\theta | \mathbf y\).

To build this approximation, we use methods like rejection sampling or Metropolis-Hastings.

** "Closeness"

We introduce two new requirements:
- a distance function: \rho : \Omega \times \Omega \rightarrow \(\mathbb R\)
- a tolerance: \epsilon : \(\mathbb R\).

Then we accept \theta if \(\rho(\mathbf x, \mathbf y) \leq \epsilon\).

\hfill

An example \rho is \( \rho(\mathbf x, \mathbf y) = \sum_{i=1}^n (\mathbf x_i - \mathbf y_i)^2 \)

** Summary Statistics

If \Omega is high-dimensional, we will have a low acceptance rate due to the "Curse of Dimensionality".

This means any samples we generate are likely to be far apart.

\hfill

We solve this by using summary statistics instead of raw data, for example quantiles.

\[ S : \Omega^* \rightarrow \Omega ; \mathbf x \mapsto (x_{(0)}, Q_1, Q_2, Q_3, x_{(n)}) \]

\hfill

The quality of summary statistic is one of the main topics of current research in ABC.

* Inference
** Setup

Provide an inference algorithm, e.g. rejection sampling.

\hfill

Different inference algorithms depend on particular operations which drive the algorithm.

\hfill

We can provide one algorithm and several implementations of "drivers"

* Rejection Sampling
** Operations

In rejection sampling, we need to be able to propose a new sample from the prior and decide whether or not to accept it.

#+begin_src haskell
class Monad m => RSKernel k m a | k -> a where
  propose :: k -> m a
  accept :: k -> a -> m Bool
#+end_src

** Algorithm

#+begin_src haskell
rs :: RSKernel k m a => Int -> k -> m [a]
rs 0 _ = return []
rs n kernel = do
  x <- sample kernel
  a <- kernel `accept` x
  if a
    then (x:) <$> rs (n-1) kernel
    else rs (n-1) kernel
#+end_src

** ABC

A simple carrier the kernel has the minimum amount of information to be able to
generate new data and decide if it's close

#+begin_src haskell
data ABC θ ω = ABC
  { observations :: ω
  , prior :: Dist θ
  , model :: θ -> Dist ω
  , distance :: ω -> ω -> Double
  , tolerance :: Double
  , gen :: MWC.Gen RealWorld
  }
#+end_src

#+beamer: \pause

#+begin_src haskell
instance RSKernel (RSABC θ ω) IO θ where
  sample :: RSABC θ ω -> IO θ
  sample RSABC{..} = runDist prior gen

  accept :: RSABC θ ω -> θ -> IO Bool
  RSABC{..} `accept` θ = do
    x <- runDist (model θ) gen
    return $ x `distance` observations <= tolerance
#+end_src

** TODO Example
* Reading
** Reading

- Fernhead and Prangle
- Marjoram et al. 2003
- Umberto Picchini
- Blum et al
