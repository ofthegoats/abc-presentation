#+startup: beamer content

#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: Inference in Haskell for ABC
#+author: Piotr Kozicki
#+email: piotr.kozicki.2022@bristol.ac.uk
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+cite_export:

#+options: H:2
#+latex_class: beamer
#+latex_compiler: xelatex
#+latex_header: \usepackage{fontspec}
#+latex_header: \setsansfont{Fira Sans}
#+latex_header: \setmonofont{Fira Code}[Contextuals=Alternate]
#+latex_header: \usepackage{pgfplots}
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: CambridgeUS
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

# REFERENCE to insert graphics later
# #+latex_header: \titlegraphic{\includegraphics{Rplots}}

* Approximate Bayesian Computation
** Bayesian Inference

#+attr_latex: :options [Bayes' Theorem]
#+begin_theorem
\( p(a|b)p(b) = p(b|a)p(a) \).
#+end_theorem

#+beamer: \pause

Often rewritten with hypotheses \theta and evidence \(\mathbf x\)

\[
\overbrace{p(\theta | \mathbf x)}^{\text{posterior}}
\propto
\overbrace{p(\mathbf x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}.
\]

** Example

Suppose \(\mathbf x = (3~1~7~5~1~2~3~4~3~2)\) comes from \(Poi(\theta)\)
assuming nothing about \(\theta\).

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\begin{align*}
  p (\theta | \mathbf x)
  & \propto p (\mathbf x | \theta) p (\theta) \\
  & \propto p (\mathbf x | \theta) = \prod_{i=1}^{10} p (\mathbf x_i | \theta) \\
  & \propto e^{-10 \theta} \theta^{31}
\end{align*}
#+end_export

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\center
\begin{tikzpicture}[scale=0.7]
\begin{axis} [
    axis lines = left,
    xlabel = {\(\theta\)},
    ylabel = {\(k \cdot p(\theta | \mathbf x)\)},
  ]
  \addplot [
    domain = 0:10,
    samples = 200,
    color = red,
  ]
  { (exp (-x * 10))
    * x^(31) };
  \addplot [color = black] coordinates {(3,0)(3,60)};
\end{axis}
\end{tikzpicture}
#+end_export

** Motivation

In this example we used a simple distribution.
In reality, the likelihood may be unavailable or intractable.

#+beamer: \pause

\hfill

To avoid this problem, we avoid likelihoods all together and instead consider
only generative models.

#+beamer: \pause

#+begin_src haskell
newtype Sampler ω = Sampler {runSampler :: ReaderT Gen IO ω}
  deriving (Functor, Applicative, Monad)

sample :: Sampler ω -> Gen -> IO ω
sample = runReaderT . runSampler
#+end_src

** Example Samplers

We can define our own Samplers by transforming existing Samplers.

#+begin_src haskell
uniform :: Double -> Double -> Sampler Double
uniform a b | a <= b = (\x -> (b - a) * x + a) <$> random
#+end_src

#+beamer: \pause

#+begin_src haskell
bernoulli :: Prob -> Sampler Bool
bernoulli p = (<=p) <$> uniform 0 1
#+end_src

#+beamer: \pause

#+begin_src haskell
binomial :: Int -> Prob -> Sampler Int
binomial n p = length . filter id
  <$> replicateM n (bernoulli p)
#+end_src

#+beamer: \pause

#+begin_src haskell
gaussian :: Double -> Double -> Sampler Double
gaussian μ σ² = (\u₁ u₂ ->
  μ + σ² * sqrt (-2 * log u₁) * cos (2 * pi * u₂))
  <$> random <*> random
#+end_src

** ABC Description

To be able to use ABC we need:
#+beamer: \pause
- A generative model \(\mu : \theta \to \texttt{Sampler}~\omega\)
#+beamer: \pause
- Observations \(\mathbf y : \omega\)

#+beamer: \pause

\hfill

To approximate \(p(\theta | \mathbf y)\)
#+beamer: \pause
we consider \(\theta_0\)
#+beamer: \pause
and take \(\mathbf x \leftarrow \mu (\theta_0)\)
#+beamer: \pause
which we compare with \(\mathbf y\)
#+beamer: \pause
to apply a weight to \(\theta_0\).

#+beamer: \pause

\hfill

To approximate the posterior, this is repeated many times for different
\(\theta\) using a Monte Carlo method.

* Rejection Sampling
** Rejection Sampling

A simple Monte-Carlo sampling method.

Proposals are taken from a prior, then we have to decide whether or not to accept them.

#+beamer: \pause

#+begin_src haskell
class RSKernel k a | k -> a where
  propose :: k -> Sampler a
  accepts :: k -> a -> Sampler Bool
#+end_src

#+beamer: \pause

#+begin_src haskell
rs :: RSKernel k a => Int -> k -> IO [a]
rs 0 _ = return []
rs n k = do
  x <- propose k
  a <- k `accepts` x
  if a
    then (x:) <$> rs (n-1) k
    else rs (n-1) k
#+end_src

** Distribution Approximation

We provide a prior \(g\) we can sample from directly such that \(f \leq M \cdot g\).

#+begin_src haskell
data RSMC ω = RSMC
  { prior :: Sampler ω
  , priorDensity :: ω -> Double -- ^ scaled by M
  , targetDensity :: ω -> Double }
#+end_src

#+beamer: \pause

#+begin_src haskell
instance RSKernel (RSMC ω) ω where
  propose :: RSMC ω -> Sampler ω
  propose RSMC{..} = prior

  accepts :: RSMC ω -> ω -> Sampler Bool
  accepts RSMC{..} x = let
    α = targetDensity x / priorDensity x
    in bernoulli $ min 1 α
#+end_src

** Approximate Bayesian Computation

#+begin_src haskell
data RSABC θ ω = RSABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , prior :: Sampler θ }
#+end_src

#+beamer: \pause

#+begin_src haskell
instance RSKernel (RSABC θ ω) θ where
  propose :: RSABC θ ω -> Sampler θ
  propose RSABC{..} = prior

  accepts :: RSABC θ ω -> θ -> Sampler Bool
  accepts RSABC{..} θ = do
    x <- model θ
    return $ x == observations
#+end_src

* Improvements
** Tolerance

To increase the acceptance rate, we usually use a weaker condition, that
\(|| \mathbf x - \mathbf y || \leq \epsilon\).

#+begin_src haskell
RSABC θ ω = RSABC
  { distance :: ω -> ω -> Double
  , tolerance :: Double
  , ... }
#+end_src

#+beamer: \pause

#+begin_src haskell
instance RSKernel (RSABC θ ω) where
  accepts :: RSABC θ ω -> θ -> Sampler Bool
  accepts RSABC{..} θ = do
    x <- model θ
    return $ x `distance` observations <= tolerance
#+end_src

#+beamer: \pause

A sensible =distance= might be a sum of [weighted] squared distances.

** Summary Statistics

We rarely compare only one sample at a time, so usually the sample space \(\omega\) will have many dimensions.

#+beamer: \pause

This introduces the "curse of dimensionality", which here means any two samples we generate might seem to be far apart.

#+beamer: \pause

\hfill

We try to solve this by replacing raw data with summary statistics[fn:2].

#+beamer: \pause

Ideally we have /sufficient/ summary statistics, i.e. \(S\) such that \(p(\theta | S(y)) = p(\theta | y)\).

#+beamer: \pause

But this almost never happens, so we instead look for "informative" summaries[fn:1], e.g. quantiles are usually fine.

* Metropolis-Hastings
** Metropolis-Hastings

An improvement on rejection sampling. Stay near to accepted samples by carrying
out a /random walk/ over values of \(\theta\), rather than resampling from the
prior.

#+beamer: \pause

#+begin_src haskell
class MHKernel k a | k -> a where
  perturb :: k -> a -> Sampler a
  accepts :: k -> a -> a -> Sampler Bool
#+end_src

#+beamer: \pause

#+begin_src haskell
mh :: MHKernel k a => Int -> k -> a -> Sampler [a]
mh 0 _ _ = return []
mh n k x_0 = do
  x_1 <- k `perturb` x_0
  a <- accepts k x_0 x_1
  if a
    then (x_1:) <$> mh (n-1) k x_1
    else (x_0:) <$> mh (n-1) k x_0
#+end_src

** Approximate Bayesian Computation

#+begin_src haskell
data MHABC θ ω = MHABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , prior :: θ -> Double -- ^ density
  , transition :: θ -> Sampler θ -- ^ assumed symmetrical
  , distance :: ω -> ω -> Double
  , tolerance :: Double }
#+end_src

#+beamer: \pause

#+begin_src haskell
instance MHKernel (MHABC θ ω) θ where
  perturb :: MHABC θ ω -> θ -> Sampler θ
  perturb MHABC{..} = transition

  accepts :: MHABC θ ω -> θ -> θ -> Sampler Bool
  accepts MHABC{..} θ θ' = do
    x <- model θ'
    if distance x observations <= tolerance
      then bernoulli $ min 1 (prior θ' / prior θ)
      else return False
#+end_src

** Possible Improvements

1. A tuning system, for adaptive Metropolis
2. Multiple particles for MCMC

If we start on a bad point, we are not likely to move. Adaptive Metropolis may
change that by increasing/decreasing the variance.

Having mutliple particles should also offset this problem.

* Reading
** Reading

- [[https://www.pnas.org/doi/10.1073/pnas.0306899100][Marjoram et al]]
- [[https://www.maths.lu.se/fileadmin/maths/forskning_research/InferPartObsProcess/abc_slides.pdf][Umberto Picchini's slides on ABC]]

- [[https://arxiv.org/abs/1004.1112][Fernhead and Prangle --- Constructing Summary Statistics]]
- [[https://projecteuclid.org/journals/statistical-science/volume-28/issue-2/A-Comparative-Review-of-Dimension-Reduction-Methods-in-Approximate-Bayesian/10.1214/12-STS406.full][Blum et al --- Comparative Review of Dimension Reduction Methods]]

- [[https://link.springer.com/article/10.1007/s11222-022-10092-4][Drovandi et Frazier --- Comparison with Full Data Methods]]

* Footnotes
[fn:2] Some (recent?) approaches to full-data ABC

[fn:1] Key problem in ABC!
