#+startup: beamer content

#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: Inference in Haskell for Approximate Bayesian Computation
#+beamer_header: \title[Inference in Haskell for ABC]{Inference in Haskell for Approximate Bayesian Computation}
#+author: Piotr Kozicki
#+email: piotr.kozicki.2022@bristol.ac.uk
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+cite_export:

#+options: H:2
#+latex_class: beamer
#+latex_compiler: xelatex
#+latex_header: \usepackage{fontspec}
#+latex_header: \setsansfont{Fira Sans}
#+latex_header: \setmonofont{Fira Mono}
#+latex_header: \usepackage{pgfplots}
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: CambridgeUS
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

# REFERENCE to insert graphics later
# #+latex_header: \titlegraphic{\includegraphics{Rplots}}

* Approximate Bayesian Computation
** Bayesian Inference

#+attr_latex: :options [Bayes' Theorem]
#+begin_theorem
\( p(a|b)p(b) = p(b|a)p(a) \).
#+end_theorem

#+beamer: \pause

Often rewritten with hypotheses \theta and evidence \(\mathbf x\)

\[
\overbrace{p(\theta | \mathbf x)}^{\text{posterior}}
\propto
\overbrace{p(\mathbf x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}.
\]

- The /posterior/ is the target
- The /likelihood/ is the model
- The /prior/ is an assumption

** Example

Suppose \(\mathbf x = (3~1~7~5~1~2~3~4~3~2)\) comes from \(Poi(\theta)\)
assuming nothing about \(\theta\).

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\begin{align*}
  p (\theta | \mathbf x)
  & \propto p (\mathbf x | \theta) p (\theta) \\
  & \propto p (\mathbf x | \theta) = \prod_{i=1}^{10} p (\mathbf x_i | \theta) \\
  & \propto e^{-10 \theta} \theta^{31}
\end{align*}
#+end_export

*** :BMCOL:B_column:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: column
:END:

#+beamer: \pause

#+begin_export latex
\center
\begin{tikzpicture}[scale=0.7]
\begin{axis} [
    axis lines = left,
    xlabel = {\(\theta\)},
    ylabel = {\(k \cdot p(\theta | \mathbf x)\)},
  ]
  \addplot [
    domain = 0:10,
    samples = 200,
    color = red,
  ]
  { (exp (-x * 10))
    * x^(31) };
  \addplot [color = black] coordinates {(3,0)(3,60)};
\end{axis}
\end{tikzpicture}
#+end_export

** Motivation

In this example we used a simple distribution.
In reality, the likelihood may be unavailable (e.g. Tukey's g-k distribution[fn:1]) or intractable (e.g. many convolutions).

#+beamer: \pause

\hfill

Avoid likelihoods! Instead use simulation with numerical and Monte-Carlo methods.

\hfill

Write a program to simulate data. Requires a source of randomness.

#+beamer: \pause

#+begin_src haskell
type Gen = MWC.GenIO

newtype Sampler ω = { runSampler :: ReaderT Gen IO ω }
  deriving (Functor, Applicative, Monad)

sample :: Sampler ω -> Gen -> IO ω
sample = runReaderT . runSampler
#+end_src

** Example Samplers

Can re-define them straight from =MWC=

#+begin_src haskell
random :: Sampler Double
random = Sampler $ ask >>= MWC.uniform -- U(0, 1)
#+end_src

#+beamer: \pause

Or we can reuse samplers to make new distributions.

#+begin_src haskell
uniform :: Double -> Double -> Sampler Double
uniform a b | a <= b = (\x -> (b - a) * x + a) <$> random

bernoulli :: Double -> Sampler Bool
bernoulli p | 0 <= p && p <= 1 = (<=p) <$> random

gaussian :: Double -> Double -> Sampler Double
gaussian μ σ² = (\u1 u2 ->
   μ + σ² * sqrt (-2 * log u1) * cos (2 * pi * u2))
  <$> random <*> random
#+end_src

** Approximate Bayesian Computation

ABC is a /likelihood-free/ method useable with =Sampler=.

\hfill

To use ABC we need:
#+beamer: \pause
- A generative model \(\mu : \theta \to \texttt{Sampler}~\omega\)
#+beamer: \pause
- Observations \(\mathbf y : \omega\)

#+beamer: \pause

\hfill

To approximate \(p(\theta | \mathbf y)\)
#+beamer: \pause
we consider \(\theta_0\)
#+beamer: \pause
and take \(\mathbf x \leftarrow \mu (\theta_0)\)
#+beamer: \pause
which we compare with \(\mathbf y\)
#+beamer: \pause
to apply a weight to \(\theta_0\).

#+beamer: \pause

\hfill

To approximate the posterior, this is repeated many times for different
\(\theta\) using a Monte-Carlo method.
* Rejection Sampling
** Rejection Sampling

A simple Monte-Carlo method. Sample \(\mathbf x\) from the prior and see how "good" it is.

Either \(\mathbf x\) is fully accepted or not, i.e. no weights besides 0 and 1.

#+beamer: \pause

\hfill

Abstracted the key operations into a "handler" =RSKernel=.

Now the algorithm is /generally/ written:

#+begin_src haskell
rs :: RSKernel k a => Int -> k -> Sampler [a]
rs 0 _ = return []
rs n k = do
  x <- propose k
  a <- k `accepts` x
  if a
    then (x:) <$> rs (n-1) k
    else rs (n-1) k
#+end_src

** Approximate Bayesian Computation

To enable ABC via rejection sampling, just need to provide a handler.

#+begin_src haskell
data RSABC θ ω = RSABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , prior :: Sampler θ }

instance Eq ω => RSKernel (RSABC θ ω) θ where
  propose :: RSABC θ ω -> Sampler θ
  propose RSABC{..} = prior

  accepts :: RSABC θ ω -> θ -> Sampler Bool
  accepts RSABC{..} θ = do
    x <- model θ
    return $ x == observations
#+end_src

* Necessary Improvements
** Tolerance

To increase the acceptance rate, we usually use a weaker condition, that
\(|| \mathbf x - \mathbf y || \leq \epsilon\).

#+begin_src haskell
RSABC θ ω = RSABC
  { distance :: ω -> ω -> Double
  , ϵ :: Double
  , ... }

instance RSKernel (RSABC θ ω) where
  ...
  accepts RSABC{..} θ = do
    x <- model θ
    return $ x `distance` observations <= ϵ
#+end_src

#+beamer: \pause

This is only /strictly/ necessary for continuous distributions.

** Dimension Reduction

We rarely use only one observation, so \(\mathbf y\) is a long vector.

\hfill

Affected by the "Curse of Dimensionality." Results that \(\mathbf x\) and \(\mathbf y\) almost always far apart.

\hfill

Solve this with /dimension reduction methods/[fn:2] e.g. *summary statistics*

** Summary Statistics

Ideally, summary \(S\) is "sufficient", i.e. \(p(\theta|S(\mathbf y)) = p(\theta|\mathbf y)\).

\hfill

Sufficient summaries are hard to find. In practice \(S\) is "informative".

\hfill

Often \(S\) maps raw data to e.g. mean, variance, quantiles...

* Metropolis-Hastings
** Metropolis-Hastings

An improvement on rejection sampling, by staying near accepted samples.

Since we stay in "good" regions and move out of "bad" regions, we will approximate the posterior sooner.

#+beamer: \pause

#+begin_src haskell
class MHKernel k a | k -> a where
  perturb :: k -> a -> Sampler a
  accepts :: k -> a -> a -> Sampler Bool

mh :: MHKernel k a => Int -> k -> a -> Sampler [a]
mh 0 _ _ = return []
mh n k last = do
  proposed <- k `perturb` last
  a <- accepts k last proposed
  if a
    then (proposed:) <$> mh (n-1) k proposed
    else (last:) <$> mh (n-1) k last
#+end_src

** Approximate Bayesian Computation

#+begin_src haskell
data MHABC θ ω = MHABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , priorD :: θ -> Double -- ^ density
  , transition :: θ -> Sampler θ -- ^ assumed symmetrical
  , distance :: ω -> ω -> Double , ϵ :: Double }

instance MHKernel (MHABC θ ω) θ where
  perturb :: MHABC θ ω -> θ -> Sampler θ
  perturb MHABC{..} = transition

  accepts :: MHABC θ ω -> θ -> θ -> Sampler Bool
  accepts MHABC{..} θ θ' = do
    x <- model θ'
    if x `distance` observations <= ϵ
      then bernoulli $ min 1 (priorD θ' / priorD θ)
      else return False
#+end_src

* Conclusion
** Summary

- Implemented Approximate Bayesian Computation, particularly for the Tukey g-and-k distribution.

- Learnt some Monte-Carlo methods and tried to implement them generally.

** Possible Developments

- Parallelisation[fn:4]

\hfill

- Adaptive Metropolis
- Particle Filter and other algorithms
- Allow p.d.f-based distributions[fn:3]

\hfill

- Use kernel density estimate with approximated posterior
- Find peaks with AD/stochastic gradient ascent

* Further Reading
** Further Reading

- [[https://www.pnas.org/doi/10.1073/pnas.0306899100][Marjoram et al]]
- [[https://www.maths.lu.se/fileadmin/maths/forskning_research/InferPartObsProcess/abc_slides.pdf][Umberto Picchini's slides]]

- [[https://arxiv.org/abs/1004.1112][Fernhead and Prangle --- Constructing Summary Statistics]]
- [[https://projecteuclid.org/journals/statistical-science/volume-28/issue-2/A-Comparative-Review-of-Dimension-Reduction-Methods-in-Approximate-Bayesian/10.1214/12-STS406.full][Blum et al --- Comparative Review of Dimension Reduction Methods]]

- [[https://link.springer.com/article/10.1007/s11222-022-10092-4][Drovandi et Frazier --- Comparison with Full Data Methods]]

* Footnotes

[fn:1] Like Gaussian distribution with skew and kurtosis.

[fn:3] Possible by Monte-Carlo methods

[fn:2] Some full-data approaches, with recent interest

[fn:4] Attempts were made... anyone?
