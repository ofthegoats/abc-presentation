#+startup: beamer content

#+options: ':t *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: Inference in Haskell for ABC
#+author: Piotr Kozicki
#+email: piotr.kozicki.2022@bristol.ac.uk
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+cite_export:

#+options: H:2
#+latex_class: beamer
#+latex_compiler: xelatex
#+latex_header: \usepackage{fontspec}
#+latex_header: \setsansfont{Fira Sans}
#+latex_header: \setmonofont{Fira Code}[Contextuals=Alternate]
#+columns: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+beamer_theme: CambridgeUS
#+beamer_color_theme:
#+beamer_font_theme:
#+beamer_inner_theme:
#+beamer_outer_theme:
#+beamer_header:

# REFERENCE to insert graphics later
# #+latex_header: \titlegraphic{\includegraphics{Rplots}}

* Approximate Bayesian Computation
** Bayesian Inference

#+attr_latex: :options [Bayes' Theorem]
#+begin_theorem
\( p(a|b)p(b) = p(b|a)p(a) \).
#+end_theorem

Often rewritten as

\[
\overbrace{p(\theta | x)}^{\text{posterior}}
\propto
\overbrace{p(x | \theta)}^{\text{likelihood}}
\overbrace{p(\theta)}^{\text{prior}}.
\]

We get a distribution on hypotheses \theta given evidence \(x\).

** TODO Example
** Motivation

In this example we used a simple distribution.
In reality, especially in science, the likelihood may be very expensive to compute.

\hfill

To avoid this problem, we avoid likelihoods all together and instead consider
only generative models.[fn:1]

#+begin_src haskell
newtype Sampler ω = Sampler {runSampler :: ReaderT Gen IO ω}
  deriving (Functor, Applicative, Monad)

sample :: Sampler ω -> Gen -> IO ω
sample = runReaderT . runSampler
#+end_src

** ABC Description

To be able to use ABC we need:
1. A generative model \(\mu : \theta \to \texttt{Sampler}~\omega\)
2. Real observations \(\mathbf y : \omega\)

#+begin_src haskell
accept :: Eq ω => ω -> Sampler ω -> θ -> Sampler Bool
accept y μ θ = do
  x <- μ θ
  return $ x == y
#+end_src

By repeating this, we can generate samples from the posterior \(p(\theta|\mathbf y)\).

#+beamer: \pause

\hfill

TODO example showing the problem

We're not usually likely to generate a sample /exactly/ the same as our observations, so we get too low an acceptance rate.

** Closeness

To increase the acceptance rate, we usually use a weaker condition, that
\(\mathbf x \approx \mathbf y\), by providing a distance function and a
tolerance \(\epsilon\).

#+begin_src haskell
accept :: ω -> Sampler ω -> θ -> Double -> Sampler Bool
accept y μ θ ϵ = do
  x <- μ θ
  return $ distance x y <= ϵ
#+end_src

The choice of distance function doesn't matter too much, so long as it's
sensible. A good choice might be the sum of squared distances: naturally this depends on \(\omega\).

\[
\rho (\mathbf x, \mathbf y)
=
\sum_{i=1}^n (\mathbf x_i - \mathbf y_i)^2
\]

** Summary Statistics

We rarely compare only one sample at a time, so usually the sample space \(\omega\) will have many dimensions.

This introduces the "curse of dimensionality", which here means any two samples we generate might seem to be far apart.

#+beamer: \pause

\hfill

We try to solve this by replacing raw data with summary statistics, e.g. quantiles.

\[
S : \omega \to s ; ~ \mathbf x \mapsto (\mathbf x_{(0)}, Q_1, Q_2, Q_3, \mathbf x_{(n)})
\]

Finding informative summary statistics is one of the bigger problems in ABC.
** Sampling Methods

Since ABC just gives us a way to decide whether or not to accept a value
\(\theta\), we still need a way to produce and accumulate them.

For this we can use Monte Carlo methods, such as rejection sampling or
Metropolis-Hastings.

* Rejection Sampling
** Rejection Sampling

A simple Monte-Carlo sampling method.

Proposals are taken from a prior, then we have to decide whether or not to accept them.

#+beamer: \pause

#+begin_src haskell
class RSKernel k a | k -> a where
  propose :: k -> IO a
  accepts :: k -> a -> IO Bool
#+end_src

#+beamer: \pause

#+begin_src haskell
rs :: RSKernel k a => Int -> k -> IO [a]
rs 0 _ = return []
rs n k = do
  x <- propose k
  a <- k `accepts` x
  if a
    then (x:) <$> rs (n-1) k
    else rs (n-1) k
#+end_src

** Distribution Approximation

We provide a prior \(g\) we can sample from directly such that \(f \leq M \cdot g\).

#+begin_src haskell
data RSMC ω = RSMC
  { prior :: Sampler ω
  , priorDensity :: ω -> Double -- ^ scaled by M
  , targetDensity :: ω -> Double
  , gen :: Gen
  }

instance RSKernel (RSMC ω) ω where
  propose :: RSMC ω -> IO ω
  propose RSMC{..} = sample prior gen

  accepts :: RSMC ω -> ω -> IO Bool
  accepts RSMC{..} x = let
    α = targetDensity x / priorDensity x
    in sample (bernoulli $ min 1 α) gen
#+end_src

*demo*

** Approximate Bayesian Computation

#+begin_src haskell
data RSABC θ ω = RSABC
  { observations :: ω
  , model :: θ -> Sampler ω
  , prior :: Sampler θ
  , distance :: ω -> ω -> Double
  , tolerance :: Double
  , gen :: Gen
  }

instance RSKernel (RSABC θ ω) θ where
  propose :: RSABC θ ω -> IO θ
  propose RSABC{..} = sample prior gen

  accepts :: RSABC θ ω -> θ -> IO Bool
  accepts RSABC{..} θ = do
    x <- sample (model θ) gen
    return $ distance x observations <= tolerance
#+end_src

*demo*

* TODO Metropolis-Hastings
** Metropolis-Hastings

Notice that with rejection sampling we reject a large amount of samples.

We can improve by staying near to "good" samples. This also means performing a random walk instead of resampling from the prior.

** Algorithm
** ABC
* Reading
** Reading

- [[https://www.pnas.org/doi/10.1073/pnas.0306899100][Marjoram et al]]
- [[https://www.maths.lu.se/fileadmin/maths/forskning_research/InferPartObsProcess/abc_slides.pdf][Umberto Picchini's slides on ABC]]

- [[https://arxiv.org/abs/1004.1112][Fernhead and Prangle --- Constructing Summary Statistics]]
- [[https://projecteuclid.org/journals/statistical-science/volume-28/issue-2/A-Comparative-Review-of-Dimension-Reduction-Methods-in-Approximate-Bayesian/10.1214/12-STS406.full][Blum et al --- Comparative Review of Dimension Reduction Methods]]

* Footnotes

[fn:1] Extend to generative models /and/ density functions?
